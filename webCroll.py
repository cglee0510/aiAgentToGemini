import requests
import feedparser
import pandas as pd
from datetime import datetime, timedelta
import time
import re
import random
from bs4 import BeautifulSoup

# ì „ì—­ ë³€ìˆ˜ë¡œ ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ê³¼ ìºì‹œ ê´€ë¦¬
last_crawl_time = None
cached_news = None
CACHE_DURATION = 60  # 60ì´ˆ ìºì‹œ ì§€ì†ì‹œê°„

def get_rss_feeds():
    """í•œêµ­ ì£¼ìš” ê²½ì œ ë‰´ìŠ¤ RSS í”¼ë“œ ëª©ë¡"""
    rss_feeds = [
        {
            'name': 'í•œêµ­ê²½ì œ',
            'url': 'https://www.hankyung.com/feed/economy',
            'category': 'ê²½ì œ'
        },
        {
            'name': 'ì—°í•©ë‰´ìŠ¤ ê²½ì œ',
            'url': 'https://www.yna.co.kr/rss/economy.xml',
            'category': 'ê²½ì œ'
        },
        {
            'name': 'ë§¤ì¼ê²½ì œ',
            'url': 'https://www.mk.co.kr/rss/30000042/',
            'category': 'ê²½ì œ'
        },
        {
            'name': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
            'url': 'https://rss.mt.co.kr/mt_newsflash.xml',
            'category': 'ì†ë³´'
        },
        {
            'name': 'ì¡°ì„ ë¹„ì¦ˆ',
            'url': 'https://biz.chosun.com/rss/finance.xml',
            'category': 'ê¸ˆìœµ'
        }
    ]
    return rss_feeds

def crawl_rss_news(max_articles=20):
    """RSS í”¼ë“œë¡œ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    
    print(f"ğŸ“° RSS í”¼ë“œë¡œ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... (ìµœëŒ€ {max_articles}ê°œ)")
    
    all_articles = []
    rss_feeds = get_rss_feeds()
    
    for feed_info in rss_feeds:
        try:
            print(f"   ğŸ“¡ {feed_info['name']} RSS ìˆ˜ì§‘ ì¤‘...")
            
            # RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(feed_info['url'])
            
            if feed.bozo:
                print(f"   âš ï¸ {feed_info['name']} RSS íŒŒì‹± ì˜¤ë¥˜: {feed.bozo_exception}")
                continue
            
            if not feed.entries:
                print(f"   âš ï¸ {feed_info['name']} RSSì— í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            print(f"   âœ… {feed_info['name']}ì—ì„œ {len(feed.entries)}ê°œ í•­ëª© ë°œê²¬")
            
            # ê° ê¸°ì‚¬ ì²˜ë¦¬
            feed_count = 0
            for entry in feed.entries:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title = entry.title if hasattr(entry, 'title') else 'ì œëª© ì—†ìŒ'
                    title = re.sub(r'<[^>]+>', '', title).strip()  # HTML íƒœê·¸ ì œê±°
                    
                    # ë§í¬ ì¶”ì¶œ
                    link = entry.link if hasattr(entry, 'link') else ''
                    
                    # ìš”ì•½ ì¶”ì¶œ
                    summary = ''
                    if hasattr(entry, 'summary'):
                        summary = entry.summary
                        summary = re.sub(r'<[^>]+>', '', summary).strip()  # HTML íƒœê·¸ ì œê±°
                        if len(summary) > 200:
                            summary = summary[:200] + "..."
                    
                    if not summary:
                        summary = f"{title}ì— ê´€í•œ {feed_info['name']} ë‰´ìŠ¤ì…ë‹ˆë‹¤."
                    
                    # ë°œí–‰ì¼ ì¶”ì¶œ
                    pub_date = datetime.now().strftime("%Y-%m-%d %H:%M")
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        try:
                            pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M")
                        except:
                            pass
                    elif hasattr(entry, 'published'):
                        pub_date = entry.published[:16] if len(entry.published) > 16 else entry.published
                    
                    # ê²½ì œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                    economic_keywords = [
                        'ê²½ì œ', 'ê¸ˆìœµ', 'ì£¼ì‹', 'ë¶€ë™ì‚°', 'ì¦ì‹œ', 'í™˜ìœ¨', 'GDP', 'ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 
                        'ê¸°ì—…', 'ì‚°ì—…', 'ë¬´ì—­', 'íˆ¬ì', 'ì‹œì¥', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ì¬ì •', 'ì˜ˆì‚°',
                        'ì€í–‰', 'ëŒ€ì¶œ', 'ì ê¸ˆ', 'í€ë“œ', 'ë³´í—˜', 'ì¹´ë“œ', 'ê²°ì œ', 'í•€í…Œí¬',
                        'ìˆ˜ì¶œ', 'ìˆ˜ì…', 'ê´€ì„¸', 'í†µìƒ', 'FTA', 'ì œì¡°ì—…', 'ì„œë¹„ìŠ¤ì—…',
                        'ë°˜ë„ì²´', 'ìë™ì°¨', 'ì¡°ì„ ', 'ì² ê°•', 'í™”í•™', 'ë°”ì´ì˜¤', 'IT', 'AI',
                        'ë¶€ê°€ì„¸', 'ì†Œë“ì„¸', 'ë²•ì¸ì„¸', 'ì„¸ê¸ˆ', 'ì„¸ìœ¨', 'ì¡°ì„¸',
                        'ì±„ê¶Œ', 'êµ­ì±„', 'íšŒì‚¬ì±„', 'ìˆ˜ìµë¥ ', 'ë°°ë‹¹',
                        'ì†Œë¹„', 'ë¬¼ê°€', 'ê°€ê²©', 'ë¹„ìš©', 'ë§¤ì¶œ', 'ìˆ˜ìµ', 'ì†ì‹¤',
                        'ê³ ìš©', 'ì·¨ì—…', 'ì‹¤ì—…', 'ì¼ìë¦¬', 'ì„ê¸ˆ', 'ì—°ë´‰',
                        'ì •ë¶€', 'í•œêµ­ì€í–‰', 'ê¸ˆìœµìœ„', 'ê¸°íšì¬ì •ë¶€',
                        'ë‹¬ëŸ¬', 'ì›í™”', 'ìœ ë¡œ', 'ì—”í™”', 'ìœ„ì•ˆí™”'
                    ]
                    
                    # ì œëª©ì— ê²½ì œ í‚¤ì›Œë“œê°€ ìˆê±°ë‚˜, ê²½ì œ ì¹´í…Œê³ ë¦¬ RSSì¸ ê²½ìš° í¬í•¨
                    has_economic_keyword = any(keyword in title for keyword in economic_keywords)
                    is_economic_category = feed_info['category'] in ['ê²½ì œ', 'ê¸ˆìœµ']
                    
                    if has_economic_keyword or is_economic_category:
                        article = {
                            'ì œëª©': title,
                            'ìš”ì•½': summary,
                            'ë°œí–‰ì¼': pub_date,
                            'URL': link,
                            'ì¶œì²˜': feed_info['name']
                        }
                        
                        all_articles.append(article)
                        feed_count += 1
                        
                        print(f"     ğŸ“ ìˆ˜ì§‘: {title[:50]}...")
                        
                        # í”¼ë“œë‹¹ ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ
                        if feed_count >= 5:
                            break
                
                except Exception as e:
                    print(f"     âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"   âœ… {feed_info['name']}ì—ì„œ {feed_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # í”¼ë“œ ê°„ ê°„ê²©
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            print(f"   âŒ {feed_info['name']} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            continue
    
    # ë°œí–‰ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    if all_articles:
        try:
            all_articles.sort(key=lambda x: x['ë°œí–‰ì¼'], reverse=True)
        except:
            pass
    
    # ìµœëŒ€ ê°œìˆ˜ ì œí•œ
    if len(all_articles) > max_articles:
        all_articles = all_articles[:max_articles]
    
    print(f"âœ… ì´ {len(all_articles)}ê±´ì˜ ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # DataFrame ìƒì„±
    if len(all_articles) == 0:
        print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=['ì œëª©', 'ìš”ì•½', 'ë°œí–‰ì¼', 'URL', 'ì¶œì²˜'])
    
    try:
        df = pd.DataFrame(all_articles)
        return df
    except Exception as e:
        print(f"âŒ DataFrame ìƒì„± ì‹¤íŒ¨: {e}")
        return pd.DataFrame(columns=['ì œëª©', 'ìš”ì•½', 'ë°œí–‰ì¼', 'URL', 'ì¶œì²˜'])

def filter_economic_news(df):
    """ê²½ì œ ë‰´ìŠ¤ í•„í„°ë§ (RSSëŠ” ì´ë¯¸ í•„í„°ë§ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ê°„ë‹¨í•œ ì²˜ë¦¬)"""
    try:
        if df.empty:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        print(f"ğŸ” RSS ë‰´ìŠ¤ ìˆ˜: {len(df)}ê±´")
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.fillna('')
        
        # ë¬¸ìì—´ ë³€í™˜
        for col in df.columns:
            df[col] = df[col].astype(str)
        
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        before_count = len(df)
        df = df.drop_duplicates(subset=['ì œëª©'], keep='first')
        after_count = len(df)
        
        if before_count != after_count:
            print(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {before_count}ê±´ â†’ {after_count}ê±´")
        
        print(f"âœ… ìµœì¢… ë‰´ìŠ¤ ìˆ˜: {len(df)}ê±´")
        return df
        
    except Exception as e:
        print(f"âŒ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        return df

def create_sample_economic_news():
    """í¬ë¡¤ë§ ì‹¤íŒ¨ì‹œ ìƒ˜í”Œ ê²½ì œ ë‰´ìŠ¤ ë°ì´í„° ìƒì„±"""
    sample_news = [
        {
            'ì œëª©': 'í•œêµ­ì€í–‰, ê¸°ì¤€ê¸ˆë¦¬ ë™ê²° ê²°ì •...ê²½ì œ ë¶ˆí™•ì‹¤ì„± ê³ ë ¤',
            'ìš”ì•½': 'í•œêµ­ì€í–‰ì´ ì´ë²ˆë‹¬ ê¸ˆìœµí†µí™”ìœ„ì›íšŒì—ì„œ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ í˜„ ìˆ˜ì¤€ì—ì„œ ë™ê²°í•˜ê¸°ë¡œ ê²°ì •í–ˆë‹¤. ëŒ€ë‚´ì™¸ ê²½ì œ ë¶ˆí™•ì‹¤ì„±ê³¼ ì¸í”Œë ˆì´ì…˜ ì••ë ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ê²°ê³¼ë‹¤.',
            'ë°œí–‰ì¼': datetime.now().strftime("%Y-%m-%d %H:%M"),
            'URL': 'https://example.com/news/1',
            'ì¶œì²˜': 'í•œêµ­ê²½ì œ'
        },
        {
            'ì œëª©': 'ì½”ìŠ¤í”¼ 2600ì„  íšŒë³µ...ì™¸êµ­ì¸ ë§¤ìˆ˜ì„¸ ì§€ì†',
            'ìš”ì•½': 'ì½”ìŠ¤í”¼ ì§€ìˆ˜ê°€ ì¥ì¤‘ 2600ì„ ì„ íšŒë³µí•˜ë©° ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆë‹¤. ì™¸êµ­ì¸ íˆ¬ììë“¤ì˜ ì§€ì†ì ì¸ ë§¤ìˆ˜ì„¸ê°€ ì§€ìˆ˜ ìƒìŠ¹ì„ ê²¬ì¸í–ˆë‹¤ëŠ” ë¶„ì„ì´ë‹¤.',
            'ë°œí–‰ì¼': datetime.now().strftime("%Y-%m-%d %H:%M"),
            'URL': 'https://example.com/news/2',
            'ì¶œì²˜': 'ì—°í•©ë‰´ìŠ¤'
        },
        {
            'ì œëª©': 'ë¶€ë™ì‚° ì •ì±… ë³€í™” ì‹ í˜¸...ì •ë¶€ ê·œì œ ì™„í™” ê²€í† ',
            'ìš”ì•½': 'ì •ë¶€ê°€ ë¶€ë™ì‚° ì‹œì¥ ì•ˆì •í™”ë¥¼ ìœ„í•´ ì¼ë¶€ ê·œì œ ì™„í™” ë°©ì•ˆì„ ê²€í† í•˜ê³  ìˆë‹¤ê³  ë°œí‘œí–ˆë‹¤. ì£¼íƒ ê³µê¸‰ í™•ëŒ€ì™€ ì‹œì¥ í™œì„±í™”ê°€ ì£¼ìš” ëª©í‘œë‹¤.',
            'ë°œí–‰ì¼': datetime.now().strftime("%Y-%m-%d %H:%M"),
            'URL': 'https://example.com/news/3',
            'ì¶œì²˜': 'ë§¤ì¼ê²½ì œ'
        },
        {
            'ì œëª©': 'ë°˜ë„ì²´ ìˆ˜ì¶œ ì¦ê°€ì„¸...ì˜¬í•´ 2ë¶„ê¸° ì‹¤ì  ê¸°ëŒ€',
            'ìš”ì•½': 'í•œêµ­ì˜ ë°˜ë„ì²´ ìˆ˜ì¶œì´ ì „ë…„ ë™ê¸° ëŒ€ë¹„ ì¦ê°€ì„¸ë¥¼ ë³´ì´ë©° ì˜¬í•´ 2ë¶„ê¸° ì‹¤ì ì— ëŒ€í•œ ê¸°ëŒ€ê°ì´ ë†’ì•„ì§€ê³  ìˆë‹¤. AI ìˆ˜ìš” ì¦ê°€ê°€ ì£¼ìš” ìš”ì¸ìœ¼ë¡œ ë¶„ì„ëœë‹¤.',
            'ë°œí–‰ì¼': datetime.now().strftime("%Y-%m-%d %H:%M"),
            'URL': 'https://example.com/news/4',
            'ì¶œì²˜': 'ì¡°ì„ ë¹„ì¦ˆ'
        },
        {
            'ì œëª©': 'í™˜ìœ¨ ì•ˆì •ì„¸...ì›/ë‹¬ëŸ¬ 1,380ì›ëŒ€ ìœ ì§€',
            'ìš”ì•½': 'ì›/ë‹¬ëŸ¬ í™˜ìœ¨ì´ 1,380ì›ëŒ€ì—ì„œ ì•ˆì •ì„¸ë¥¼ ë³´ì´ê³  ìˆë‹¤. ë¯¸ ì—°ì¤€ì˜ í†µí™”ì •ì±… ë³€í™” ê¸°ëŒ€ê°ê³¼ êµ­ë‚´ ìˆ˜ì¶œ í˜¸ì¡°ê°€ ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ëŠ” ë¶„ì„ì´ë‹¤.',
            'ë°œí–‰ì¼': datetime.now().strftime("%Y-%m-%d %H:%M"),
            'URL': 'https://example.com/news/5',
            'ì¶œì²˜': 'ë¨¸ë‹ˆíˆ¬ë°ì´'
        }
    ]
    
    return pd.DataFrame(sample_news)

def generate_per_report():
    """ê²½ì œ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„± (RSS ê¸°ë°˜ + ìºì‹œ ì‹œìŠ¤í…œ)"""
    global last_crawl_time, cached_news
    
    try:
        print("ğŸ“Š ê²½ì œ ë‰´ìŠ¤ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
        
        # ìºì‹œ í™•ì¸ (60ì´ˆ ì´ë‚´ë©´ ìºì‹œ ì‚¬ìš©)
        current_time = datetime.now()
        if (last_crawl_time and cached_news is not None and 
            (current_time - last_crawl_time).seconds < CACHE_DURATION):
            
            remaining_time = CACHE_DURATION - (current_time - last_crawl_time).seconds
            print(f"ğŸ”„ ìºì‹œëœ ë°ì´í„° ì‚¬ìš© (ê°±ì‹ ê¹Œì§€ {remaining_time}ì´ˆ ë‚¨ìŒ)")
            economic_news = cached_news
        else:
            # ìƒˆë¡œ í¬ë¡¤ë§
            print("ğŸ• ìƒˆë¡œìš´ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # í¬ë¡¤ë§ ê°„ê²© ì²´í¬ (ìµœì†Œ 30ì´ˆ ê°„ê²© ë³´ì¥ - RSSëŠ” ëœ ì œí•œì )
            if last_crawl_time:
                time_diff = (current_time - last_crawl_time).seconds
                if time_diff < 30:
                    wait_time = 30 - time_diff
                    print(f"â³ ì ì ˆí•œ ê°„ê²©ì„ ìœ„í•´ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
            
            # RSS í¬ë¡¤ë§ ìˆ˜í–‰
            news_df = crawl_rss_news(max_articles=15)
            
            if news_df.empty:
                if cached_news is not None:
                    print("âš ï¸ ìƒˆ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ - ì´ì „ ìºì‹œ ë°ì´í„° ì‚¬ìš©")
                    economic_news = cached_news
                else:
                    print("âš ï¸ RSS ìˆ˜ì§‘ ì‹¤íŒ¨ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
                    economic_news = create_sample_economic_news()
            else:
                # ê²½ì œ ë‰´ìŠ¤ í•„í„°ë§
                economic_news = filter_economic_news(news_df)
                
                if economic_news.empty:
                    if cached_news is not None:
                        print("âš ï¸ í•„í„°ë§ëœ ë‰´ìŠ¤ ì—†ìŒ - ì´ì „ ìºì‹œ ë°ì´í„° ì‚¬ìš©")
                        economic_news = cached_news
                    else:
                        print("âš ï¸ í•„í„°ë§ ê²°ê³¼ ì—†ìŒ - ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
                        economic_news = create_sample_economic_news()
                else:
                    # ìºì‹œ ì—…ë°ì´íŠ¸
                    cached_news = economic_news.copy()
                    last_crawl_time = current_time
                    print(f"âœ… ìƒˆ ë°ì´í„° ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"economic_news_report_{timestamp}.txt"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"ğŸ“Š ê²½ì œ ê¸°ì‚¬ ë¦¬í¬íŠ¸ ({timestamp} ê¸°ì¤€)\n")
            f.write(f"{'='*60}\n\n")
            f.write(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(economic_news)}ê±´\n")
            f.write(f"ğŸ• ìƒì„± ì‹œê°„: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„ %Sì´ˆ')}\n")
            f.write(f"ğŸ“¡ ë°ì´í„° ì†ŒìŠ¤: RSS í”¼ë“œ (í•œêµ­ê²½ì œ, ì—°í•©ë‰´ìŠ¤, ë§¤ì¼ê²½ì œ, ë¨¸ë‹ˆíˆ¬ë°ì´, ì¡°ì„ ë¹„ì¦ˆ)\n")
            
            # ìºì‹œ ìƒíƒœ í‘œì‹œ
            if last_crawl_time:
                cache_age = (current_time - last_crawl_time).seconds
                f.write(f"ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„: {last_crawl_time.strftime('%H:%M:%S')} ({cache_age}ì´ˆ ì „)\n\n")
            else:
                f.write(f"ğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„: ë°©ê¸ˆ ì „\n\n")
            
            # ì¶œì²˜ë³„ í†µê³„
            if 'ì¶œì²˜' in economic_news.columns:
                source_counts = economic_news['ì¶œì²˜'].value_counts()
                f.write("ğŸ“Š ì¶œì²˜ë³„ ê¸°ì‚¬ ìˆ˜:\n")
                for source, count in source_counts.items():
                    f.write(f"   - {source}: {count}ê±´\n")
                f.write("\n")
            
            for idx, row in economic_news.iterrows():
                f.write(f"[{idx+1}] {row['ì œëª©']}\n")
                f.write(f"    ğŸ“… ë°œí–‰ì¼: {row['ë°œí–‰ì¼']}\n")
                f.write(f"    ğŸ“ ìš”ì•½: {row['ìš”ì•½']}\n")
                f.write(f"    ğŸ”— ë§í¬: {row['URL']}\n")
                if 'ì¶œì²˜' in row:
                    f.write(f"    ğŸ“° ì¶œì²˜: {row['ì¶œì²˜']}\n")
                f.write(f"    {'-'*50}\n\n")
        
        print(f"âœ… {len(economic_news)}ê±´ì˜ ê²½ì œ ê¸°ì‚¬ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {filename}")
        return filename
        
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

# ì´ì „ í•¨ìˆ˜ë“¤ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
def crawl_naver_news(query="ê²½ì œ", max_pages=1):
    """í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ - RSS í¬ë¡¤ë§ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    print("ğŸ“¡ RSS í”¼ë“œ ë°©ì‹ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    return crawl_rss_news(max_articles=10)